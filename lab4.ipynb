{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Лабораторная работа №4: Проведение исследований со случайным лесом",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Задача классификации",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 2. Создание бейзлайна и оценка качества",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X = bl_cdata.drop('HeartDisease', axis=1) # Все столбцы кроме 'HeartDisease' - это признаки\ny = bl_cdata['HeartDisease'] # 'HeartDisease' - это целевая переменная, которую мы хотим предсказать\n\n# Разделяем данные на обучающую и тестовую выборки\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Масштабируем признаки, это важно для некоторых моделей (хотя для случайного леса это не обязательно, но оставим для согласованности)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train) # Масштабируем обучающие данные\nX_test_scaled = scaler.transform(X_test) # Масштабируем тестовые данные\n\n# Создаем модель случайного леса\nmodel = RandomForestClassifier(random_state=42)\n\n# Обучаем модель на масштабированных обучающих данных\nmodel.fit(X_train_scaled, y_train)\n\n# Делаем предсказания на масштабированных тестовых данных\ny_pred = model.predict(X_test_scaled)\n\n# Вычисляем метрики\naccuracy = accuracy_score(y_test, y_pred) # Доля правильных ответов\nprecision = precision_score(y_test, y_pred, average='weighted', zero_division=0) # Точность предсказаний для каждого класса, взвешенная\nrecall = recall_score(y_test, y_pred, average='weighted', zero_division=0) # Полнота предсказаний для каждого класса, взвешенная\n\n# Выводим метрики\nprint(f\"Метрики для классификации (Случайный лес):\")\nprint(f\"  Accuracy: {accuracy:.4f}\")\nprint(f\"  Precision: {precision:.4f}\")\nprint(f\"  Recall: {recall:.4f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "  Accuracy: 0.8859\n  Precision: 0.8880\n  Recall: 0.8859\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 3. Улучшение бейзлайна",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Подбор оптимальных гиперпараметров для модели случайного леса с помощью GridSearchCV улучшит ее производительность по сравнению с использованием параметров по умолчанию. Гиперпараметры модели существенно влияют на ее способность к обобщению и точности предсказаний. Поиск наилучших параметров через GridSearchCV позволит обучить модель более эффективно.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X = bl_cdata.drop('HeartDisease', axis=1)\ny = bl_cdata['HeartDisease']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\ndef evaluate_model_cv(model, X, y, cv=2): \n    scores_acc = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n    scores_prec = cross_val_score(model, X, y, cv=cv, scoring='precision_weighted', error_score=0)\n    scores_rec = cross_val_score(model, X, y, cv=cv, scoring='recall_weighted', error_score=0)\n    return np.mean(scores_acc), np.mean(scores_prec), np.mean(scores_rec)\n\n\nbase_rf = RandomForestClassifier(random_state=42)\nbase_rf_acc, base_rf_prec, base_rf_rec = evaluate_model_cv(base_rf, X_train_scaled, y_train)\n\n\nparam_distributions = {\n    'n_estimators': [100, 200, 300, 500],\n    'max_depth': [5, 10, 15, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\nrandom_search = RandomizedSearchCV(\n    RandomForestClassifier(random_state=42),\n    param_distributions,\n    n_iter=10,  \n    cv=2,  \n    scoring='accuracy',\n    n_jobs=-1,\n    random_state=42\n)\nrandom_search.fit(X_train_scaled, y_train)\n\nbest_rf = random_search.best_estimator_\nbest_params = random_search.best_params_\nbest_rf_acc, best_rf_prec, best_rf_rec = evaluate_model_cv(best_rf, X_train_scaled, y_train)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Обучаем модели с лучшими параметрами",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "best_rf_improved = RandomForestClassifier(random_state=42, **best_params)\nbest_rf_improved.fit(X_train_scaled, y_train)\nbest_rf_improved_y_pred = best_rf_improved.predict(X_test_scaled)\n\nbest_rf_improved_accuracy = accuracy_score(y_test, best_rf_improved_y_pred)\nbest_rf_improved_precision = precision_score(y_test, best_rf_improved_y_pred, average='weighted', zero_division=0)\nbest_rf_improved_recall = recall_score(y_test, best_rf_improved_y_pred, average='weighted', zero_division=0)\n\nprint(\"\\nМетрики улучшенной модели на тестовых данных:\")\nprint(f\"  Улучшенный Случайный лес:\")\nprint(f\"  Accuracy: {best_rf_improved_accuracy:.4f}, Precision: {best_rf_improved_precision:.4f}, Recall: {best_rf_improved_recall:.4f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "Метрики улучшенной модели на тестовых данных:\n  Улучшенный Случайный лес:\n  Accuracy: 0.8859, Precision: 0.8901, Recall: 0.8859",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Можно заметить небольшое улучшение на Precision",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 4. Имплементация алгоритма машинного обучения",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Напишем собственную реализацию решающего дерева для классификации и регрессии",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class Node:\n    \"\"\"Узел дерева решений.\"\"\"\n    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n        self.feature = feature  # Индекс признака для разбиения\n        self.threshold = threshold  # Пороговое значение для разбиения\n        self.left = left  # Левый дочерний узел\n        self.right = right  # Правый дочерний узел\n        self.value = value  # Значение в листовом узле\n\nclass DecisionTree:\n    \"\"\"Дерево решений для регрессии и классификации.\"\"\"\n    def __init__(self, min_samples_split=2, max_depth=100, mode = 'classification'):\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        self.root = None\n        self.mode = mode # 'classification' or 'regression'\n\n    def _entropy(self, y):\n        \"\"\"Вычисляет энтропию для классификации.\"\"\"\n        hist = np.bincount(y)\n        ps = hist / len(y)\n        return -np.sum([p * np.log2(p) for p in ps if p > 0])\n\n    def _variance(self, y):\n       \"\"\"Вычисляет дисперсию для регрессии.\"\"\"\n       if len(y) == 0:\n         return 0\n       return np.var(y)\n\n    def _information_gain(self, y, y_left, y_right):\n        \"\"\"Вычисляет прирост информации.\"\"\"\n        if self.mode == 'classification':\n           p = len(y_left) / len(y)\n           return self._entropy(y) - p * self._entropy(y_left) - (1 - p) * self._entropy(y_right)\n        elif self.mode == 'regression':\n          p = len(y_left) / len(y)\n          return self._variance(y) - p * self._variance(y_left) - (1-p) * self._variance(y_right)\n\n\n    def _best_split(self, X, y):\n        \"\"\"Находит наилучшее разбиение для заданных данных.\"\"\"\n        best_gain = -1\n        split_idx, split_thresh = None, None\n\n        for feature_idx in range(X.shape[1]):\n            X_column = X[:, feature_idx]\n            thresholds = np.unique(X_column)\n            for threshold in thresholds:\n                y_left = y[X_column <= threshold]\n                y_right = y[X_column > threshold]\n                if len(y_left) == 0 or len(y_right) == 0:\n                    continue\n                gain = self._information_gain(y, y_left, y_right)\n\n                if gain > best_gain:\n                    best_gain = gain\n                    split_idx = feature_idx\n                    split_thresh = threshold\n\n        return split_idx, split_thresh\n\n    def _build_tree(self, X, y, depth=0):\n        \"\"\"Рекурсивно строит дерево решений.\"\"\"\n        n_samples, n_features = X.shape\n\n        # Критерии остановки\n        if depth >= self.max_depth or n_samples < self.min_samples_split or len(np.unique(y)) == 1:\n           if self.mode == 'classification':\n               most_common = Counter(y).most_common(1)[0][0]\n               return Node(value=most_common)\n           elif self.mode == 'regression':\n               return Node(value=np.mean(y))\n\n\n        best_feature, best_thresh = self._best_split(X, y)\n\n        if best_feature is None: # Не нашли хорошего разбиения\n             if self.mode == 'classification':\n               most_common = Counter(y).most_common(1)[0][0]\n               return Node(value=most_common)\n             elif self.mode == 'regression':\n                return Node(value=np.mean(y))\n\n\n        left_idxs = X[:, best_feature] <= best_thresh\n        right_idxs = X[:, best_feature] > best_thresh\n\n        left_child = self._build_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n        right_child = self._build_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n\n        return Node(feature=best_feature, threshold=best_thresh, left=left_child, right=right_child)\n\n\n    def fit(self, X, y):\n        \"\"\"Обучает дерево решений.\"\"\"\n        self.root = self._build_tree(X, y)\n\n    def _predict_one(self, x, node):\n       \"\"\"Предсказывает значение для одного экземпляра.\"\"\"\n       if node.value is not None:\n           return node.value\n\n       if x[node.feature] <= node.threshold:\n           return self._predict_one(x, node.left)\n       else:\n           return self._predict_one(x, node.right)\n\n\n    def predict(self, X):\n        \"\"\"Предсказывает значения для набора данных.\"\"\"\n        return [self._predict_one(x, self.root) for x in X]\n\n\n\nclass RandomForest:\n    \"\"\"Случайный лес для классификации и регрессии.\"\"\"\n    def __init__(self, n_trees=10, max_features='sqrt', min_samples_split=2, max_depth=100, mode='classification'):\n        self.n_trees = n_trees\n        self.max_features = max_features # 'sqrt' or int\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        self.trees = []\n        self.mode = mode # 'classification' or 'regression'\n\n\n    def fit(self, X, y):\n        \"\"\"Обучает случайный лес.\"\"\"\n        self.trees = []\n        n_features = X.shape[1]\n\n        if self.max_features == 'sqrt':\n            n_feat_sub = int(np.sqrt(n_features))\n        elif isinstance(self.max_features, int):\n           n_feat_sub = self.max_features\n        else:\n           n_feat_sub = n_features # использовать все признаки\n\n\n        for _ in range(self.n_trees):\n            # Bootstrap агрегирование (случайная выборка с возвращением)\n            idxs = np.random.choice(len(y), len(y), replace=True)\n            X_sample = X[idxs]\n            y_sample = y[idxs]\n\n            # Случайный выбор подмножества признаков\n            feat_idxs = np.random.choice(n_features, n_feat_sub, replace=False)\n            X_sample_subset = X_sample[:, feat_idxs]\n\n\n            tree = DecisionTree(min_samples_split = self.min_samples_split, max_depth=self.max_depth, mode=self.mode)\n            tree.fit(X_sample_subset, y_sample)\n            self.trees.append((tree, feat_idxs))\n\n\n    def predict(self, X):\n        \"\"\"Предсказывает значения для набора данных.\"\"\"\n        if not self.trees:\n           raise ValueError(\"Случайный лес не обучен. Сначала вызовите fit.\")\n\n        predictions = np.zeros((X.shape[0], self.n_trees))\n\n        for i, (tree, feat_idxs) in enumerate(self.trees):\n            predictions[:, i] = tree.predict(X[:, feat_idxs])\n\n\n        if self.mode == 'classification':\n          final_predictions = np.array([Counter(row).most_common(1)[0][0] for row in predictions])\n        elif self.mode == 'regression':\n          final_predictions = np.mean(predictions, axis=1) # Среднее предсказание всех деревьев\n\n        return final_predictions\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "X_clf = bl_cdata.drop(\"HeartDisease\", axis=1).values\ny_clf = bl_cdata[\"HeartDisease\"].values\n\nX_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42)\n\nrf_clf = RandomForest(n_trees=100, mode='classification')\nrf_clf.fit(X_train_clf, y_train_clf)\ny_pred_clf = rf_clf.predict(X_test_clf)\n\n\naccuracy = accuracy_score(y_test_clf, y_pred_clf)\nrecall = recall_score(y_test_clf, y_pred_clf)\nprecision = precision_score(y_test_clf, y_pred_clf)\n\n\nprint(\"\\nКлассификация (Heart Disease Dataset):\")\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Recall: {recall}\")\nprint(f\"Precision: {precision}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "Accuracy: 0.8858\nPrecision: 0.9134\nRecall: 0.8878\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Результаты лучше, чем у библиотечной реализации",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Задача регрессии",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 2. Создание бейзлайна и оценка качества",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X = bl_rdata.drop(target_variable, axis=1)\ny = bl_rdata[target_variable]\n\n\ny_reg = y.astype(float)\n\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y_reg, test_size=0.2, random_state=42)\n\n\nscaler = StandardScaler()\nX_train_scaled_reg = scaler.fit_transform(X_train_reg)\nX_test_scaled_reg = scaler.transform(X_test_reg)\n\n\nmodel_reg = RandomForestRegressor(random_state=42)\n\n\nmodel_reg.fit(X_train_scaled_reg, y_train_reg)\n\n\ny_pred_reg = model_reg.predict(X_test_scaled_reg)\n\n\nmae = mean_absolute_error(y_test_reg, y_pred_reg) \nr2 = r2_score(y_test_reg, y_pred_reg) \n\n\nprint(f\"\\nМетрики для регрессии (Случайный лес):\")\nprint(f\"  MAE: {mae:.4f}\")\nprint(f\"  R2: {r2:.4f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "  MAE: 0.1571\n  R2: 0.7764",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 3. Улучшение бейзлайна",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "Воспользуемся тем же методом, что и в случае с классификацией",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "X = bl_cdata.drop('HeartDisease', axis=1)\ny = bl_cdata['HeartDisease'].astype(float)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [5, 10, 15],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\ngrid_search = GridSearchCV(RandomForestRegressor(random_state=42),\n                            param_grid,\n                            cv=3,  \n                            scoring='neg_mean_absolute_error',  \n                            n_jobs=-1) \n\ngrid_search.fit(X_train_scaled, y_train)\n\nbest_params = grid_search.best_params_\nbest_score = -grid_search.best_score_ # Возвращаем положительный MAE\nprint(f\"Лучшие гиперпараметры: {best_params}\")\nprint(f\"Лучший MAE на кросс-валидации: {best_score:.4f}\")\n\n\nbest_model = RandomForestRegressor(**best_params, random_state=42)\nbest_model.fit(X_train_scaled, y_train)\ny_pred = best_model.predict(X_test_scaled)\n\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"\\nМетрики на тестовой выборке с лучшими гиперпараметрами:\")\nprint(f\"  MAE: {mae:.4f}\")\nprint(f\"  R2: {r2:.4f}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "  MAE: 0.2061\n  R2: 0.6102",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Результат стал хуже в сравнении с изначальной версией",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 4. Имплементация алгоритма машинного обучения",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Воспользуемся теми же классами, что использовались для задачи регрессии",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class Node:\n    \"\"\"Узел дерева решений.\"\"\"\n    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n        self.feature = feature  # Индекс признака для разбиения\n        self.threshold = threshold  # Пороговое значение для разбиения\n        self.left = left  # Левый дочерний узел\n        self.right = right  # Правый дочерний узел\n        self.value = value  # Значение в листовом узле\n\nclass DecisionTree:\n    \"\"\"Дерево решений для регрессии и классификации.\"\"\"\n    def __init__(self, min_samples_split=2, max_depth=100, mode = 'classification'):\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        self.root = None\n        self.mode = mode # 'classification' or 'regression'\n\n    def _entropy(self, y):\n        \"\"\"Вычисляет энтропию для классификации.\"\"\"\n        hist = np.bincount(y)\n        ps = hist / len(y)\n        return -np.sum([p * np.log2(p) for p in ps if p > 0])\n\n    def _variance(self, y):\n       \"\"\"Вычисляет дисперсию для регрессии.\"\"\"\n       if len(y) == 0:\n         return 0\n       return np.var(y)\n\n    def _information_gain(self, y, y_left, y_right):\n        \"\"\"Вычисляет прирост информации.\"\"\"\n        if self.mode == 'classification':\n           p = len(y_left) / len(y)\n           return self._entropy(y) - p * self._entropy(y_left) - (1 - p) * self._entropy(y_right)\n        elif self.mode == 'regression':\n          p = len(y_left) / len(y)\n          return self._variance(y) - p * self._variance(y_left) - (1-p) * self._variance(y_right)\n\n\n    def _best_split(self, X, y):\n        \"\"\"Находит наилучшее разбиение для заданных данных.\"\"\"\n        best_gain = -1\n        split_idx, split_thresh = None, None\n\n        for feature_idx in range(X.shape[1]):\n            X_column = X[:, feature_idx]\n            thresholds = np.unique(X_column)\n            for threshold in thresholds:\n                y_left = y[X_column <= threshold]\n                y_right = y[X_column > threshold]\n                if len(y_left) == 0 or len(y_right) == 0:\n                    continue\n                gain = self._information_gain(y, y_left, y_right)\n\n                if gain > best_gain:\n                    best_gain = gain\n                    split_idx = feature_idx\n                    split_thresh = threshold\n\n        return split_idx, split_thresh\n\n    def _build_tree(self, X, y, depth=0):\n        \"\"\"Рекурсивно строит дерево решений.\"\"\"\n        n_samples, n_features = X.shape\n\n        # Критерии остановки\n        if depth >= self.max_depth or n_samples < self.min_samples_split or len(np.unique(y)) == 1:\n           if self.mode == 'classification':\n               most_common = Counter(y).most_common(1)[0][0]\n               return Node(value=most_common)\n           elif self.mode == 'regression':\n               return Node(value=np.mean(y))\n\n\n        best_feature, best_thresh = self._best_split(X, y)\n\n        if best_feature is None: # Не нашли хорошего разбиения\n             if self.mode == 'classification':\n               most_common = Counter(y).most_common(1)[0][0]\n               return Node(value=most_common)\n             elif self.mode == 'regression':\n                return Node(value=np.mean(y))\n\n\n        left_idxs = X[:, best_feature] <= best_thresh\n        right_idxs = X[:, best_feature] > best_thresh\n\n        left_child = self._build_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n        right_child = self._build_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n\n        return Node(feature=best_feature, threshold=best_thresh, left=left_child, right=right_child)\n\n\n    def fit(self, X, y):\n        \"\"\"Обучает дерево решений.\"\"\"\n        self.root = self._build_tree(X, y)\n\n    def _predict_one(self, x, node):\n       \"\"\"Предсказывает значение для одного экземпляра.\"\"\"\n       if node.value is not None:\n           return node.value\n\n       if x[node.feature] <= node.threshold:\n           return self._predict_one(x, node.left)\n       else:\n           return self._predict_one(x, node.right)\n\n\n    def predict(self, X):\n        \"\"\"Предсказывает значения для набора данных.\"\"\"\n        return [self._predict_one(x, self.root) for x in X]\n\n\n\nclass RandomForest:\n    \"\"\"Случайный лес для классификации и регрессии.\"\"\"\n    def __init__(self, n_trees=10, max_features='sqrt', min_samples_split=2, max_depth=100, mode='classification'):\n        self.n_trees = n_trees\n        self.max_features = max_features # 'sqrt' or int\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        self.trees = []\n        self.mode = mode # 'classification' or 'regression'\n\n\n    def fit(self, X, y):\n        \"\"\"Обучает случайный лес.\"\"\"\n        self.trees = []\n        n_features = X.shape[1]\n\n        if self.max_features == 'sqrt':\n            n_feat_sub = int(np.sqrt(n_features))\n        elif isinstance(self.max_features, int):\n           n_feat_sub = self.max_features\n        else:\n           n_feat_sub = n_features # использовать все признаки\n\n\n        for _ in range(self.n_trees):\n            # Bootstrap агрегирование (случайная выборка с возвращением)\n            idxs = np.random.choice(len(y), len(y), replace=True)\n            X_sample = X[idxs]\n            y_sample = y[idxs]\n\n            # Случайный выбор подмножества признаков\n            feat_idxs = np.random.choice(n_features, n_feat_sub, replace=False)\n            X_sample_subset = X_sample[:, feat_idxs]\n\n\n            tree = DecisionTree(min_samples_split = self.min_samples_split, max_depth=self.max_depth, mode=self.mode)\n            tree.fit(X_sample_subset, y_sample)\n            self.trees.append((tree, feat_idxs))\n\n\n    def predict(self, X):\n        \"\"\"Предсказывает значения для набора данных.\"\"\"\n        if not self.trees:\n           raise ValueError(\"Случайный лес не обучен. Сначала вызовите fit.\")\n\n        predictions = np.zeros((X.shape[0], self.n_trees))\n\n        for i, (tree, feat_idxs) in enumerate(self.trees):\n            predictions[:, i] = tree.predict(X[:, feat_idxs])\n\n\n        if self.mode == 'classification':\n          final_predictions = np.array([Counter(row).most_common(1)[0][0] for row in predictions])\n        elif self.mode == 'regression':\n          final_predictions = np.mean(predictions, axis=1) # Среднее предсказание всех деревьев\n\n        return final_predictions\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "X_reg = bl_rdata.drop(\"Depression_Yes\", axis=1).values\ny_reg = bl_rdata[\"Depression_Yes\"].values\n\n\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n\nrf_reg = RandomForest(n_trees=100, mode='regression')\nrf_reg.fit(X_train_reg, y_train_reg)\ny_pred_reg = rf_reg.predict(X_test_reg)\n\n\nmae = mean_absolute_error(y_test_reg, y_pred_reg)\nr2 = r2_score(y_test_reg, y_pred_reg)\n\n\nprint(\"Регрессия (Depression Dataset):\")\nprint(f\"MAE: {MAE}\")\nprint(f\"R2: {R2}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "  MAE: 0.2376\n  R2: 0.2244",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Результат снова стал хуже",
      "metadata": {}
    }
  ]
}