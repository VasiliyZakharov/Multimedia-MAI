{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Лабораторная работа №5: Проведение исследований с градиентным бустингом",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 2. Создание бейзлайна и оценка качества",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Классификация",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X_c = bl_cdata.drop('HeartDisease', axis=1) \n\nX_c_train, X_c_test, y_c_train, y_c_test = train_test_split(X_c, y_c, test_size=0.2, random_state=42)\n\n\nscaler_c = StandardScaler()\nX_c_train_scaled = scaler_c.fit_transform(X_c_train) \n\nX_c_test_scaled = scaler_c.transform(X_c_test) \n\nmodel_gb_c = GradientBoostingClassifier(random_state=42)\n\n\nmodel_gb_c.fit(X_c_train_scaled, y_c_train)\n\n\ny_c_pred = model_gb_c.predict(X_c_test_scaled)\n\n\naccuracy_c = accuracy_score(y_c_test, y_c_pred) \nprecision_c = precision_score(y_c_test, y_c_pred, average='weighted', zero_division=0) \nrecall_c = recall_score(y_c_test, y_c_pred, average='weighted', zero_division=0)\n\n\nprint(f\"\\nМетрики для классификации (Градиентный бустинг, bl_cdata):\")\nprint(f\"  Accuracy: {accuracy_c:.4f}\")\nprint(f\"  Precision: {precision_c:.4f}\")\nprint(f\"  Recall: {recall_c:.4f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "  Accuracy: 0.8804\n  Precision: 0.8820\n  Recall: 0.8804",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Регрессия",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X_r = bl_rdata.drop('Depression_Yes', axis=1) \ny_r = bl_rdata['Depression_Yes'] \n\nX_r_train, X_r_test, y_r_train, y_r_test = train_test_split(X_r, y_r, test_size=0.2, random_state=42)\n\n\nscaler_r = StandardScaler()\nX_r_train_scaled = scaler_r.fit_transform(X_r_train) \nX_r_test_scaled = scaler_r.transform(X_r_test) \n\n\nmodel_gb_r = GradientBoostingRegressor(random_state=42)\n\n\nmodel_gb_r.fit(X_r_train_scaled, y_r_train)\n\ny_r_pred = model_gb_r.predict(X_r_test_scaled)\n\n\nmae_r = mean_absolute_error(y_r_test, y_r_pred) \nr2_r = r2_score(y_r_test, y_r_pred)\n\nprint(f\"\\nМетрики для регрессии (Градиентный бустинг, bl_rdata):\")\nprint(f\"  MAE: {mae_r:.4f}\")\nprint(f\"  R2: {r2_r:.4f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "  MAE: 0.2185\n  R2: 0.7108",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 3. Улучшение бейзлайна",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Подбор оптимальных гиперпараметров для модели случайного леса с помощью GridSearchCV улучшит ее производительность по сравнению с использованием параметров по умолчанию. Гиперпараметры модели существенно влияют на ее способность к обобщению и точности предсказаний. Поиск наилучших параметров через GridSearchCV позволит обучить модель более эффективно.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Для классификации",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X_c = bl_cdata.drop('HeartDisease', axis=1)\ny_c = bl_cdata['HeartDisease']\n\n\nX_c_train, X_c_test, y_c_train, y_c_test = train_test_split(X_c, y_c, test_size=0.2, random_state=42)\n\n\nscaler_c = StandardScaler()\nX_c_train_scaled = scaler_c.fit_transform(X_c_train)\nX_c_test_scaled = scaler_c.transform(X_c_test)\n\n\n\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [5, 10, 15],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\ngrid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n                            param_grid=param_grid,\n                            cv=3,  \n                            scoring='accuracy',\n                            n_jobs=-1) \ngrid_search.fit(X_c_train_scaled, y_c_train)\n\nprint(\"Best hyperparameters:\", grid_search.best_params_)\nprint(\"Best score:\", grid_search.best_score_)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "Best hyperparameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 300}\nBest score: 0.8773558603769377",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Обучаем модели с лучшими параметрами",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "best_model = RandomForestClassifier(random_state=42, **grid_search.best_params_)\n\n\nbest_model.fit(X_c_train_scaled, y_c_train)\n\ny_c_pred = best_model.predict(X_c_test_scaled)\n\n\naccuracy_c = accuracy_score(y_c_test, y_c_pred)\nprecision_c = precision_score(y_c_test, y_c_pred, average='weighted', zero_division=0)\nrecall_c = recall_score(y_c_test, y_c_pred, average='weighted', zero_division=0)\n\nprint(f\"\\nМетрики для классификации (Улучшенный бейзлайн - случайный лес):\")\nprint(f\"  Accuracy: {accuracy_c:.4f}\")\nprint(f\"  Precision: {precision_c:.4f}\")\nprint(f\"  Recall: {recall_c:.4f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "  Accuracy: 0.8859\n  Precision: 0.8857\n  Recall: 0.8859",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Для регрессии",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X_r = bl_rdata.drop('Depression_Yes', axis=1) \ny_r = bl_rdata['Depression_Yes'] \n\n\nX_r_train, X_r_test, y_r_train, y_r_test = train_test_split(X_r, y_r, test_size=0.2, random_state=42)\n\n\nscaler_r = StandardScaler()\nX_r_train_scaled = scaler_r.fit_transform(X_r_train) \nX_r_test_scaled = scaler_r.transform(X_r_test) \n\nparam_grid_r = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n\ngrid_search_r = GridSearchCV(estimator=GradientBoostingRegressor(random_state=42),\n                            param_grid=param_grid_r,\n                            cv=3,  \n                            scoring='neg_mean_absolute_error', \n                            n_jobs=-1) \n\ngrid_search_r.fit(X_r_train_scaled, y_r_train)\n\n\nprint(\"Best hyperparameters (regr):\", grid_search_r.best_params_)\nprint(\"Best score (regr):\", -grid_search_r.best_score_)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Обучаем модели с лучшими параметрами",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "  MAE: 0.1411\n  R2: 0.7122",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Улучшения видны как для классификации, так и для регрессии",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 4. Имплементация алгоритма машинного обучения",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Напишем собственную реализацию градиентного бустинга для классификации и регрессии",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class SimpleGradientBoostingClassifier:\n    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        self.max_depth = max_depth\n        self.trees = []\n\n    def fit(self, X, y):\n        self.trees = []\n        residual = y.copy().astype(float)  # Convert to float for residuals\n        for _ in range(self.n_estimators):\n            tree = DecisionTreeRegressor(max_depth=self.max_depth)  # Use regressor\n            tree.fit(X, residual)\n            self.trees.append(tree)\n            predictions = tree.predict(X)\n            residual = residual - self.learning_rate * predictions\n\n    def predict(self, X):\n        predictions = np.zeros(X.shape[0])\n        for tree in self.trees:\n            predictions += self.learning_rate * tree.predict(X)\n        return np.round(predictions)  # Apply rounding\n\n\nclass SimpleGradientBoostingRegressor:\n    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        self.max_depth = max_depth\n        self.trees = []\n\n    def fit(self, X, y):\n        self.trees = []\n        residual = y.copy().astype(float)\n        for _ in range(self.n_estimators):\n            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n            tree.fit(X, residual)\n            self.trees.append(tree)\n            predictions = tree.predict(X)\n            residual = residual - self.learning_rate * predictions\n\n    def predict(self, X):\n        predictions = np.zeros(X.shape[0])\n        for tree in self.trees:\n            predictions += self.learning_rate * tree.predict(X)\n        return predictions\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Обучим модели и оценим их качество",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "model_c_imp = SimpleGradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\nmodel_c_imp.fit(X_c_train_scaled, y_c_train)\ny_c_pred_imp = model_c_imp.predict(X_c_test_scaled)\n\naccuracy_c_imp = accuracy_score(y_c_test, y_c_pred_imp)\nprecision_c_imp = precision_score(y_c_test, y_c_pred_imp, average='weighted', zero_division=0)\nrecall_c_imp = recall_score(y_c_test, y_c_pred_imp, average='weighted', zero_division=0)\n\nprint(\"\\nCustom Gradient Boosting Classifier Metrics:\")\nprint(f\"  Accuracy: {accuracy_c_imp:.4f}\")\nprint(f\"  Precision: {precision_c_imp:.4f}\")\nprint(f\"  Recall: {recall_c_imp:.4f}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "  Accuracy: 0.8641\n  Precision: 0.8680\n  Recall: 0.8641",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "model_r_imp = SimpleGradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\nmodel_r_imp.fit(X_r_train_scaled, y_r_train)\ny_r_pred_imp = model_r_imp.predict(X_r_test_scaled)\n\nmae_r_imp = mean_absolute_error(y_r_test, y_r_pred_imp)\nr2_r_imp = r2_score(y_r_test, y_r_pred_imp)\n\nprint(\"\\nCustom Gradient Boosting Regressor Metrics:\")\nprint(f\"  MAE: {mae_r_imp:.4f}\")\nprint(f\"  R2: {r2_r_imp:.4f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "  MAE: 0.2185\n  R2: 0.7108",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Как видно имплементрованный градиентныё бустинг работает немного хуже. Добавим техники из улучшенного бейзлайна",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "param_grid_c = {\n    'n_estimators': [50, 100, 150],\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'min_samples_split': [2, 5],\n    'min_samples_leaf': [1, 2]\n}\n\nbest_params_c, best_score_c, results_c = tune_hyperparameters(X_c_train_scaled, y_c_train, X_c_test_scaled, y_c_test,\n                                                              'classification', param_grid_c)\nprint(\"Best classification params:\", best_params_c)\nprint(\"Best classification accuracy:\", best_score_c)\n\n\nparam_grid_r = {\n    'n_estimators': [50, 100, 150],\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'min_samples_split': [2, 5],\n    'min_samples_leaf': [1, 2]\n}\n\nbest_params_r, best_score_r, results_r = tune_hyperparameters(X_r_train_scaled, y_r_train, X_r_test_scaled, y_r_test,\n                                                              'regression', param_grid_r)\nprint(\"Best regression params:\", best_params_r)\nprint(\"Best regression MAE:\", best_score_r)\n\n\nbest_model_c = SimpleGradientBoostingClassifier(**best_params_c)\nbest_model_c.fit(X_c_train_scaled, y_c_train)\ny_c_pred_imp_best = best_model_c.predict(X_c_test_scaled)\n\naccuracy_c_imp_best = accuracy_score(y_c_test, y_c_pred_imp_best)\nprecision_c_imp_best = precision_score(y_c_test, y_c_pred_imp_best, average='weighted', zero_division=0)\nrecall_c_imp_best = recall_score(y_c_test, y_c_pred_imp_best, average='weighted', zero_division=0)\n\nprint(\"\\nCustom Gradient Boosting Classifier Metrics (with best params):\")\nprint(f\"  Accuracy: {accuracy_c_imp_best:.4f}\")\nprint(f\"  Precision: {precision_c_imp_best:.4f}\")\nprint(f\"  Recall: {recall_c_imp_best:.4f}\")\n\nbest_model_r = SimpleGradientBoostingRegressor(**best_params_r)\nbest_model_r.fit(X_r_train_scaled, y_r_train)\ny_r_pred_imp_best = best_model_r.predict(X_r_test_scaled)\n\nmae_r_imp_best = mean_absolute_error(y_r_test, y_r_pred_imp_best)\nr2_r_imp_best = r2_score(y_r_test, y_r_pred_imp_best)\n\nprint(\"\\nCustom Gradient Boosting Regressor Metrics (with best params):\")\nprint(f\"  MAE: {mae_r_imp_best:.4f}\")\nprint(f\"  R2: {r2_r_imp_best:.4f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "Custom Gradient Boosting Classifier Metrics (with best params):\n  Accuracy: 0.8967\n  Precision: 0.8989\n  Recall: 0.8967\n\nCustom Gradient Boosting Regressor Metrics (with best params):\n  MAE: 0.1399\n  R2: 0.5847",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Видно, что после улучшения задача классификации стала работать немного быстрее",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}